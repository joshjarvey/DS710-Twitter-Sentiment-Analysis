{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required packages.\n",
    "    #tweepy for the API and searching of twitter.\n",
    "import tweepy\n",
    "\n",
    "    #string to remove punctation, unicodedata and unidecode to convert emoji into text representation.\n",
    "import string\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "\n",
    "    #textblob for the sentiment analysis.\n",
    "from textblob import TextBlob\n",
    "\n",
    "    #pandas to store it into a dataframe, and csv to write out. \n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_tweets(term,date,retweets,lang,amount):\n",
    "    \"\"\" \n",
    "        This function creates an Oauth handler for Twitter.com's API using credentials provided in the .py file.\n",
    "        The function then connects to Twitter's API, and using the Cursor object from Tweepy package it does a \n",
    "        search for tweets using the parameters specified. The text field of these tweets are then stored into\n",
    "        a list, and returned to the caller. \n",
    "\n",
    "        Parameters: \n",
    "            term         (string): The term used to search twitter for tweets\n",
    "            date         (string): The date used for earliest search (note: free accounts can only go back 7 days)\n",
    "            retweets    (Boolean): Whether to include retweets or not. \n",
    "            lang         (string): The language of tweet to return.\n",
    "            amount          (int): The number of tweets to pull. \n",
    "\n",
    "        Returns: \n",
    "            list: A list of raw tweet information. \n",
    "\n",
    "    \"\"\"\n",
    "    include_retweets = ' -filter:retweets'\n",
    "    #enable retweets if requested by the caller\n",
    "    if retweets == True:\n",
    "        include_retweets = ''\n",
    "        \n",
    "    %run ~/twitter_credentials.py\n",
    "        #Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "    auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "    auth.set_access_token(acc_token, acc_secret)\n",
    "        #connect to the twitter API using the authentication\n",
    "    api = tweepy.API(auth)  \n",
    "        #using cursor search twitter using given parameters - extended mode so it doesnt truncate. \n",
    "    tweet_block = tweepy.Cursor(api.search, q=term+include_retweets, lang=lang, since=date, tweet_mode='extended').items(amount)\n",
    "        #pull text from cursor item\n",
    "    raw_text_tweets = [tweet.full_text for tweet in tweet_block]\n",
    "    \n",
    "    return(raw_text_tweets)\n",
    "\n",
    "raw_tweets = pull_tweets(\"FF7R\",\"2020-05-03\",False,\"en\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depunctify_dynamic(text, replacement_chars=string.punctuation):\n",
    "    \"\"\" \n",
    "    Itereates over the list of passed in replacement_chars, and for each iteration replace\n",
    "    the char with an empty string. This \"cleans\" the text based on what was passed for the\n",
    "    replacement_chars\n",
    "   \n",
    "    Parameters: \n",
    "        text                (string): The text to remove punctuation.\n",
    "        replacement_chars   (string): The string of punctuation to remove from the text.\n",
    "                                      default value is the string.punctuation value.\n",
    "        \n",
    "    Returns: \n",
    "        string: A string of the \"cleaned\" text value.\n",
    "  \n",
    "    \"\"\"\n",
    "    for pchar in replacement_chars:\n",
    "        text = text.replace(pchar,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(raw_tweets):\n",
    "    \"\"\" \n",
    "    This accepts a list of raw tweets, cleans them, and returns the processed tweets. First, it takes each tweet\n",
    "    from the raw list and splits it into individuals words as a sub list. Second, it iterates through each \n",
    "    split-word tweet, goes into each word and processes the word cleaning of punctuation and URLs. Finally\n",
    "    it rejoins the words together, strips whitespace, and rebuilds the list of cleaned tweets. \n",
    "   \n",
    "    Parameters: \n",
    "        raw_tweets  (list): A list of tweets that contain undesired formatting in their body of text\n",
    "        \n",
    "    Returns: \n",
    "        list: A list of the cleaned_tweets. \n",
    "  \n",
    "    \"\"\"\n",
    "    temp_cleaning_tweet = []\n",
    "    clean_tweets = []\n",
    "\n",
    "        #split each tweet in the raw_tweets data into its own list, but also split it by word, since we need to clean it next.\n",
    "    split_list_tweets = [tweet.split() for tweet in raw_tweets]\n",
    "\n",
    "        #iterate through each split-word tweet from the previous line.\n",
    "    for tweet in split_list_tweets:\n",
    "            temp_cleaning_list = []\n",
    "                #dig into the actual word[s] of the split-word tweet.\n",
    "            for word in tweet:\n",
    "                    #if URL, need to remove\n",
    "                if word[0:8] == \"https://\":\n",
    "                    word = \"\"\n",
    "                    #strip punctuation using the depunctify_dynamic() function\n",
    "                word = depunctify_dynamic(word,(string.punctuation.replace('?','').replace('!','') + \"‚Äù‚Äú‚Äò‚Ä¶‚Äô\"))       \n",
    "                    #add cleaned word to temp list\n",
    "                temp_cleaning_list.append(word)\n",
    "                #join each word in the temp cleaned list together, strip whitespace, then add to the final list\n",
    "            clean_tweets.append((\" \".join(temp_cleaning_list)).strip())\n",
    "\n",
    "    return(clean_tweets)\n",
    "\n",
    "cleaned_tweets = clean_tweets(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I did not create this function (although i did add comments for my edification). \n",
    "#Credit goes to: user3082900 on Stackoverflow:\n",
    "#https://stackoverflow.com/questions/43797500/python-replace-unicode-emojis-with-ascii-characters/43813727#43813727\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    returnString = \"\"\n",
    "        #iterate through each character in the tweet\n",
    "    for character in inputString:\n",
    "            #if it can be encoded to ascii, do so\n",
    "        try:\n",
    "            character.encode(\"ascii\")\n",
    "            returnString += character\n",
    "            #otherwise, if its unicode, decode the character\n",
    "        except UnicodeEncodeError:\n",
    "            replaced = unidecode(str(character))\n",
    "            if replaced != '':\n",
    "                returnString += replaced\n",
    "            else:\n",
    "                    #otherwise, pull the name of the unicode character and return this\n",
    "                try:\n",
    "                     returnString += \"[\" + unicodedata.name(character) + \"]\"\n",
    "                    #all others, return X\n",
    "                except ValueError:\n",
    "                     returnString += \"[x]\"\n",
    "        #return the decoded tweet. \n",
    "    return returnString\n",
    "\n",
    "    #iterate through each tweet and decode any unicode or emojis with its text equivalent\n",
    "cleaned_decoded_tweets = [deEmojify(tweet) for tweet in cleaned_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>the only stairs i can climb is the 59 flight o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.189660</td>\n",
       "      <td>Final thoughts on FF7R side quests were fun an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.400000</td>\n",
       "      <td>Its moments like this in DFFOO that I apprecia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.400000</td>\n",
       "      <td>FUCK YOU JULES FF7R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>Wanted to RP but I got distracted finishing FF...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>Look I just cry every time I see her and hear ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>Surprise! A little morning FF7R stream We just...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.020833</td>\n",
       "      <td>Morning bois hows everyones Sunday so far? Gon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.145833</td>\n",
       "      <td>Me after finally beating Hell House on hard mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>FINAL FANTASY VII REMAKE ORIGINAL SOUNDTRACK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Polarity                                              Tweet  isPositive\n",
       "0   0.000000  the only stairs i can climb is the 59 flight o...           0\n",
       "1   0.189660  Final thoughts on FF7R side quests were fun an...           1\n",
       "2  -0.400000  Its moments like this in DFFOO that I apprecia...           0\n",
       "3  -0.400000                                FUCK YOU JULES FF7R           0\n",
       "4   0.875000  Wanted to RP but I got distracted finishing FF...           1\n",
       "..       ...                                                ...         ...\n",
       "95  0.350000  Look I just cry every time I see her and hear ...           1\n",
       "96  0.031250  Surprise! A little morning FF7R stream We just...           1\n",
       "97  0.020833  Morning bois hows everyones Sunday so far? Gon...           1\n",
       "98 -0.145833  Me after finally beating Hell House on hard mo...           0\n",
       "99  0.187500       FINAL FANTASY VII REMAKE ORIGINAL SOUNDTRACK           1\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conduct_sentiment_analysis(cleaned_tweets):\n",
    "    \"\"\" \n",
    "    This accepts a list of cleaned tweets, and runs them through the TextBlob package for sentiment analysis\n",
    "    First it \"blobs\" each cleaned tweet (breaks down into NLP components. Second, it calculates polarity on that \n",
    "    text, and returns it along with the text back into a list. Third, a dataframe is created using the values\n",
    "    and additional binary column is added, isPositive = 1 if the polarity is greater than zero, else 0. \n",
    "   \n",
    "    Parameters: \n",
    "        cleaned_tweets  (list): A list of tweets that are ready for sentiment analysis.\n",
    "        \n",
    "    Returns: \n",
    "        DataFrame: A dataframe containing the sentiment analysis of the tweets. \n",
    "  \n",
    "    \"\"\"\n",
    "        #\"blobbing\" each tweet, adding it to a list of \"blobbed\" tweets\n",
    "    textblob_tweet = [TextBlob(tweet) for tweet in cleaned_tweets]\n",
    "        #for each blobbed item, create a list of the calculated polarity and the text of the tweet - store that into a list.\n",
    "    sentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in textblob_tweet]\n",
    "        #create a dataframe using the polarity in one column, and the tweet text in the other.\n",
    "    df = pd.DataFrame(sentiment_values, columns=[\"Polarity\",\"Tweet\"])\n",
    "        #add a binary \"isPositive\" column for any polarity greater than 0\n",
    "    df['isPositive'] = df.apply(lambda row: 1 if (row.Polarity >0) else 0, axis=1)\n",
    "\n",
    "    return(df)\n",
    "\n",
    "    #complete the sentiment analysis and store in a dataframe\n",
    "tweet_df = conduct_sentiment_analysis(cleaned_decoded_tweets)\n",
    "    #review dataframe\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØcell changed to markdown since we are not writing out to file in final submission.\n",
    "\n",
    "üéØcell was here to write out tweets in increments of 1000 over the course of 3 sessions, pulling data twice per session.\n",
    "\n",
    "üéØdata is combined to obtain the combined dataset with the code displayed in the other markdown cell below.\n",
    "\n",
    "`tweet_df.to_csv(\"C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/tweets.csv\", index=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cloud': 5, 'Tifa': 9, 'Aerith': 7, 'Sephiroth': 6, 'Barret': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def character_counter(df):\n",
    "    \"\"\" \n",
    "    This function extracts the text column from the dataframe, and splits it into a list of words per entry.\n",
    "    The function then iterates through each word (converts to lowercase) and compares the word to a list of character names\n",
    "    If the character name is a match, then its related counter is incremented. A zipped list of names and counts\n",
    "    is created, casted to a dictionary and returned to the caller. \n",
    "   \n",
    "    Parameters: \n",
    "        df (DataFrame): A dataframe containing the text of the tweets\n",
    "        \n",
    "    Returns: \n",
    "        Dictionary: A dictionary that contains the frequency counts of character names.\n",
    "  \n",
    "    \"\"\"\n",
    "        #initinalizing counters\n",
    "    cloud_count = 0\n",
    "    tifa_count = 0\n",
    "    aerith_count = 0\n",
    "    sephiroth_count = 0\n",
    "    barret_count = 0\n",
    "    names = [\"Cloud\", \"Tifa\", \"Aerith\", \"Sephiroth\", \"Barret\"]\n",
    "\n",
    "        #inner for loop creates a list of tweet text from the dataframe.\n",
    "        #outter for loop seperates each tweet into single words.\n",
    "        #this creates a nested list of words. \n",
    "    words_list = [word.split() for word in [tweet for tweet in df[\"Tweet\"]]]\n",
    "\n",
    "        #iterating through each word of each tweet and counting the number of times the name appears. \n",
    "    for tweet in words_list:\n",
    "            for word in tweet:\n",
    "                if word.lower() in (\"cloud\",\"cloudstrife\"):\n",
    "                    cloud_count +=1\n",
    "                elif word.lower() in (\"tifa\",\"tifalockhart\"):\n",
    "                    tifa_count +=1\n",
    "                elif word.lower() in (\"aerith\",\"aerithgainsborough\"):\n",
    "                    aerith_count +=1\n",
    "                elif word.lower() == \"sephiroth\":\n",
    "                    sephiroth_count +=1\n",
    "                elif word.lower() in (\"barret\",\"barretwallace\"):\n",
    "                    barret_count +=1\n",
    "\n",
    "        #building a list of the final name counts\n",
    "    counts = [cloud_count, tifa_count, aerith_count, sephiroth_count, barret_count]                \n",
    "\n",
    "        #zipping the character names and character counts, converting to dictionary, and returning.\n",
    "    return dict(zip(names,counts))\n",
    "\n",
    "character_counter(tweet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØChanging cell to markdown since final submission did not include all 6 source files in my github (just the combined file). Map the read_csv function to each .csv file. For each file read in this way, concatinate/append it to the tweet_df dataframe.\n",
    "\n",
    "`combine_tweet_df = pd.concat(map(pd.read_csv,[C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/FF7R_tweets_4_19_2020.csv\",\"C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/FF7R_tweets_4_24_2020.csv\",\"C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/FF7R_tweets_4_26_2020.csv\",\"C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/finalfantasy7remake_tweets_4_19_2020.csv\",\"C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/finalfantasy7remake_tweets_4_24_2020.csv\",\"C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/finalfantasy7remake_tweets_4_26_2020.csv\"]))` \n",
    "\n",
    "\n",
    "üéØOutput combined .csv files into one combined_dataset\n",
    "`combine_tweet_df.to_csv(\"C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/combined_dataset.csv\", index=False)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_character_frequency(df, target_output):\n",
    "    \"\"\" \n",
    "    Writes a .csv file of the number of times a character name appears in the passed in dataframe.\n",
    "   \n",
    "    Parameters: \n",
    "                df (DataFrame): The dataframe that will be parsed for character names. \n",
    "        target_output (string): The pathway and file name for data to be written.\n",
    "        \n",
    "    Returns: \n",
    "        String: A simple message that lets the user know the function was successfully executed.\n",
    "  \n",
    "    \"\"\"\n",
    "    with open(target_output, 'w', newline='') as output_file: #opening the output file to write\n",
    "        writer = csv.writer(output_file) \n",
    "        for key, value in character_counter(df).items(): #calling the character_counter() function on the dataframe, and then iterating through each key-value pair in the returned dict, and writing to the csv file.\n",
    "            writer.writerow([key,value])\n",
    "\n",
    "    return \"File Successfully Saved.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØCell changed to markdown since we are not writing out to file in final submission.\n",
    "\n",
    "üéØThis produced the character frequency counts, using the combined data set.\n",
    "\n",
    "`write_character_frequency(combine_tweet_df,\"C:/Users/joshj/Documents/GitHub/ds710spring2020finalproject/character_frequency.csv\")`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
